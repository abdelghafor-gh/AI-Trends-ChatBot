{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dotenv in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.0.5)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\admin\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\ADmiN\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install dotenv python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "import html\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import logging\n",
    "from pprint import pformat\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pymongo import MongoClient\n",
    "from pymongo.errors import OperationFailure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRSSScraper:\n",
    "    \"\"\"Base class for RSS feed scrapers.\"\"\"\n",
    "    \n",
    "    def __init__(self, feed_url: str, source_name: str):\n",
    "        \"\"\"Initialize the RSS feed scraper.\n",
    "        \n",
    "        Args:\n",
    "            feed_url (str): URL of the RSS feed\n",
    "            source_name (str): Name of the source (e.g., 'amazon_science')\n",
    "        \"\"\"\n",
    "        self.feed_url = feed_url\n",
    "        self.source_name = source_name\n",
    "        self._setup_logging()\n",
    "        self._setup_mongodb()\n",
    "    \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Set up logging for the scraper.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"rss_scraper.{self.source_name}\")\n",
    "        if not self.logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            self.logger.addHandler(handler)\n",
    "            self.logger.setLevel(logging.INFO)\n",
    "    \n",
    "    def _load_mongodb_schema(self):\n",
    "        \"\"\"Load MongoDB schema from schema.json file.\"\"\"\n",
    "        try:\n",
    "            schema_path = Path(__file__).parent / \"schema.json\"\n",
    "            with open(schema_path, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error loading MongoDB schema: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _setup_mongodb(self):\n",
    "        \"\"\"Set up MongoDB connection and ensure schema validation.\"\"\"\n",
    "        try:\n",
    "            load_dotenv()\n",
    "            self.mongo_uri = os.getenv(\"MONGO_URI\")\n",
    "            self.db_name = os.getenv(\"MONGO_DB_NAME\", \"ai_trends\")\n",
    "            self.collection_name = os.getenv(\"MONGO_COLLECTION_NAME\", \"rss_feeds\")\n",
    "            \n",
    "            if not self.mongo_uri:\n",
    "                self.logger.error(\"MongoDB URI not found in environment variables\")\n",
    "                return\n",
    "            \n",
    "            self.client = MongoClient(self.mongo_uri)\n",
    "            self.db = self.client[self.db_name]\n",
    "            \n",
    "            # Create collection with schema validation if it doesn't exist\n",
    "            if self.collection_name not in self.db.list_collection_names():\n",
    "                schema = self._load_mongodb_schema()\n",
    "                if schema:\n",
    "                    self.db.create_collection(self.collection_name, **schema)\n",
    "                else:\n",
    "                    self.logger.error(\"Failed to load MongoDB schema, collection will be created without validation\")\n",
    "                    self.db.create_collection(self.collection_name)\n",
    "            \n",
    "            self.collection = self.db[self.collection_name]\n",
    "            self.logger.info(f\"Successfully connected to MongoDB: {self.db_name}.{self.collection_name}\")\n",
    "            \n",
    "        except ConnectionError as e:\n",
    "            self.logger.error(f\"Failed to connect to MongoDB: {e}\")\n",
    "            self.client = None\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error setting up MongoDB: {e}\")\n",
    "            self.client = None\n",
    "    \n",
    "    def inspect_feed_structure(self, sample_size: int = 1) -> None:\n",
    "        \"\"\"Inspect and log the structure of the RSS feed.\n",
    "        \n",
    "        Args:\n",
    "            sample_size (int): Number of entries to inspect\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(self.feed_url)\n",
    "            response.raise_for_status()\n",
    "            feed = feedparser.parse(response.text)\n",
    "            \n",
    "            # Log feed metadata structure\n",
    "            self.logger.info(\"Feed Metadata Structure:\")\n",
    "            feed_meta = {k: v for k, v in feed.feed.items() if k not in ['entries']}\n",
    "            self.logger.info(pformat(feed_meta))\n",
    "            \n",
    "            # Log entry structure for sample entries\n",
    "            if feed.entries:\n",
    "                self.logger.info(f\"\\nSample Entry Structure (first {sample_size} entries):\")\n",
    "                for i, entry in enumerate(feed.entries[:sample_size]):\n",
    "                    self.logger.info(f\"\\nEntry {i+1}:\")\n",
    "                    self.logger.info(pformat(entry))\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error inspecting feed structure: {e}\")\n",
    "    \n",
    "    def fetch_feed(self) -> Dict:\n",
    "        \"\"\"Fetch and parse the RSS feed.\"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Fetching feed from {self.feed_url}\")\n",
    "            response = requests.get(self.feed_url)\n",
    "            response.raise_for_status()\n",
    "            feed = feedparser.parse(response.text)\n",
    "            return self._process_feed(feed)\n",
    "        except requests.RequestException as e:\n",
    "            self.logger.error(f\"Error fetching feed: {e}\")\n",
    "            return {}\n",
    "\n",
    "    def _process_feed(self, feed: feedparser.FeedParserDict) -> Dict:\n",
    "        \"\"\"Process the feed and extract relevant information.\"\"\"\n",
    "        feed_info = {\n",
    "            \"feed_title\": feed.feed.get(\"title\", \"\"),\n",
    "            \"feed_link\": feed.feed.get(\"link\", \"\"),\n",
    "            \"feed_description\": feed.feed.get(\"description\", \"\"),\n",
    "            \"feed_language\": feed.feed.get(\"language\", \"\"),\n",
    "            \"last_updated\": feed.feed.get(\"updated\", \"\"),\n",
    "            \"source\": self.source_name,\n",
    "            \"scrape_date\": datetime.utcnow().isoformat(),\n",
    "            \"entries\": []\n",
    "        }\n",
    "\n",
    "        for entry in feed.entries:\n",
    "            processed_entry = self._process_entry(entry)\n",
    "            feed_info[\"entries\"].append(processed_entry)\n",
    "\n",
    "        self.logger.info(f\"Processed {len(feed_info['entries'])} entries\")\n",
    "        return feed_info\n",
    "\n",
    "    def _process_entry(self, entry: Dict) -> Dict:\n",
    "        \"\"\"Process a single feed entry.\"\"\"\n",
    "        description = html.unescape(entry.get(\"description\", \"\"))\n",
    "        \n",
    "        # Try different date fields that might be present\n",
    "        date_fields = [\"published\", \"updated\", \"pubDate\", \"date\"]\n",
    "        pub_date = None\n",
    "        for field in date_fields:\n",
    "            pub_date = entry.get(field, \"\")\n",
    "            if pub_date:\n",
    "                break\n",
    "        \n",
    "        try:\n",
    "            if pub_date:\n",
    "                # Try different date formats\n",
    "                date_formats = [\n",
    "                    \"%a, %d %b %Y %H:%M:%S %Z\",\n",
    "                    \"%Y-%m-%dT%H:%M:%S%z\",\n",
    "                    \"%Y-%m-%dT%H:%M:%SZ\",\n",
    "                    \"%Y-%m-%d %H:%M:%S\"\n",
    "                ]\n",
    "                for fmt in date_formats:\n",
    "                    try:\n",
    "                        dt = datetime.strptime(pub_date, fmt)\n",
    "                        pub_date = dt.isoformat()\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Could not parse date: {pub_date} - {e}\")\n",
    "\n",
    "        return {\n",
    "            \"title\": entry.get(\"title\", \"\"),\n",
    "            \"link\": entry.get(\"link\", \"\"),\n",
    "            \"description\": description,\n",
    "            \"published_date\": pub_date,\n",
    "            \"guid\": entry.get(\"guid\", \"\"),\n",
    "            \"categories\": entry.get(\"tags\", []),\n",
    "            \"author\": entry.get(\"author\", \"\")\n",
    "        }\n",
    "\n",
    "    def save_to_json(self, base_path: str):\n",
    "        \"\"\"Save the scraped feed data to a JSON file with date-based organization.\n",
    "        \n",
    "        Args:\n",
    "            base_path (str): Base path for saving files\n",
    "        \"\"\"\n",
    "        feed_data = self.fetch_feed()\n",
    "        if not feed_data:\n",
    "            return\n",
    "        \n",
    "        # Create date-based directory structure\n",
    "        now = datetime.utcnow()\n",
    "        date_path = now.strftime(\"%Y/%m/%d\")\n",
    "        output_dir = Path(base_path) / self.source_name / date_path\n",
    "        output_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Create filename with timestamp\n",
    "        timestamp = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        output_file = output_dir / f\"feed_{timestamp}.json\"\n",
    "        \n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(feed_data, f, indent=2, ensure_ascii=False)\n",
    "        self.logger.info(f\"Feed data saved to {output_file}\")\n",
    "        \n",
    "    def save_to_mongodb(self) -> bool:\n",
    "        \"\"\"Save the scraped feed data to MongoDB.\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if save was successful, False otherwise\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'client') or not self.client:\n",
    "            self.logger.error(\"MongoDB client not initialized\")\n",
    "            return False\n",
    "            \n",
    "        try:\n",
    "            feed_data = self.fetch_feed()\n",
    "            if not feed_data:\n",
    "                return False\n",
    "                \n",
    "            # Add unique compound index on source and scrape_date\n",
    "            self.collection.create_index([(\"source\", 1), (\"scrape_date\", 1)], unique=True)\n",
    "            \n",
    "            # Insert or update the document\n",
    "            result = self.collection.update_one(\n",
    "                {\n",
    "                    \"source\": feed_data[\"source\"],\n",
    "                    \"scrape_date\": feed_data[\"scrape_date\"]\n",
    "                },\n",
    "                {\"$set\": feed_data},\n",
    "                upsert=True\n",
    "            )\n",
    "            \n",
    "            self.logger.info(\n",
    "                f\"Successfully saved feed data to MongoDB. \"\n",
    "                f\"Modified: {result.modified_count}, Upserted: {bool(result.upserted_id)}\"\n",
    "            )\n",
    "            return True\n",
    "            \n",
    "        except OperationFailure as e:\n",
    "            self.logger.error(f\"MongoDB operation failed: {e}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error saving to MongoDB: {e}\")\n",
    "            return False\n",
    "            \n",
    "    def get_latest_entries(self, limit: Optional[int] = None) -> List[Dict]:\n",
    "        \"\"\"Get the latest entries from the feed.\n",
    "        \n",
    "        Args:\n",
    "            limit (Optional[int]): Maximum number of entries to return\n",
    "            \n",
    "        Returns:\n",
    "            List[Dict]: List of latest entries\n",
    "        \"\"\"\n",
    "        feed_data = self.fetch_feed()\n",
    "        entries = feed_data.get(\"entries\", [])\n",
    "        \n",
    "        if limit:\n",
    "            entries = entries[:limit]\n",
    "            \n",
    "        return entries\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup MongoDB connection when object is destroyed.\"\"\"\n",
    "        if hasattr(self, 'client') and self.client:\n",
    "            self.client.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

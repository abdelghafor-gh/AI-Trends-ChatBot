{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a29ea24-876f-4edd-93ff-d586354e6adf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "configs = {\n",
    "    \"fs.azure.account.auth.type\": \"OAuth\",\n",
    "    \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "    \"fs.azure.account.oauth2.client.id\": \"<client-id>\",                # Replace with your Azure AD Application (App) Client ID\n",
    "    \"fs.azure.account.oauth2.client.secret\": \"<client-secret>\",        # Replace with your App Client Secret\n",
    "    \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/<tenant-id>/oauth2/token\" # Replace with your Azure Directory Tenant ID\n",
    "}\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=\"abfss://<container-name>@<storage-account-name>.dfs.core.windows.net/\", # you have to create a container called bronze_layer\n",
    "    mount_point=\"/mnt/bronze-layer\",\n",
    "    extra_configs=configs\n",
    ")\n",
    "\n",
    "dbutils.fs.mount(\n",
    "    source=\"abfss://<container-name-2>@<storage-account-name>.dfs.core.windows.net/\", # you have to create a container called silver_layer\n",
    "    mount_point=\"/mnt/silver-layer\",\n",
    "    extra_configs=configs\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39cc27cf-f8bc-482d-8bd0-986709657504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c08f5684-b23a-47b3-91ea-4abadc741e1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# restat the kernel if we install the langchain\n",
    "# dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40f7a11-e68f-428a-9b72-9d6a2271fa89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "\n",
    "# Specify the file path in your ADLS\n",
    "new_data_path = \"/mnt/bronze-layer/new_data.parquet\"\n",
    "\n",
    "# Check if the file exists\n",
    "try:\n",
    "    files = dbutils.fs.ls(new_data_path)\n",
    "    logging.info(\"File exists.\")\n",
    "except Exception as e:\n",
    "    logging.error(\"No New Data Found\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a33707ae-3314-491e-a3d1-2c14fbfd9437",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def process_content(content: str) -> list:\n",
    "    if content:\n",
    "        # Define the text splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1100,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len,\n",
    "            add_start_index=True,\n",
    "        )\n",
    "\n",
    "        # Process the content\n",
    "        chunks = text_splitter.create_documents([content])\n",
    "\n",
    "        # Add end_index for each chunk\n",
    "        for chunk in chunks:\n",
    "            start_index = chunk.metadata.get(\"start_index\", 0)\n",
    "            end_index = start_index + len(chunk.page_content)\n",
    "            chunk.metadata[\"end_index\"] = end_index\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54203e20-1d3d-448c-a921-24577017bce9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "from pyspark.sql.types import IntegerType,StructField,DateType\n",
    "\n",
    "def create_chunks_list(news_df,news_chunks):\n",
    "    n=0\n",
    "    structured_news_chunks = []\n",
    "\n",
    "    for news in news_df.rdd.collect():\n",
    "\n",
    "        chunks = news_chunks[n]  \n",
    "\n",
    "        for data_chunk in chunks:\n",
    "            # print(data_chunk)\n",
    "            \n",
    "            chunk ={\n",
    "                \"chunk_id\" : str(uuid.uuid4()),\n",
    "                \"guid\" : news.guid,\n",
    "                \"pub_date\":news.pub_date,\n",
    "                \"chunk_text\": data_chunk.page_content,\n",
    "                \"start_index\":data_chunk.metadata[\"start_index\"] ,\n",
    "                \"end_index\":data_chunk.metadata[\"end_index\"]  \n",
    "            }\n",
    "            structured_news_chunks.append(chunk)\n",
    "    \n",
    "    return structured_news_chunks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ed309aa-d6f8-4cb7-93d7-79ab8ed68ce2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Creating a file in ADLS\n",
    "# dbutils.fs.put(\"/mnt/bronze-layer/test.txt\", \"This is a test file.\", overwrite=True)\n",
    "# # Creating a file in ADLS\n",
    "# dbutils.fs.rm(\"/mnt/bronze-layer/test.txt\", recurse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "462b0199-8a8e-439b-8fa2-ac0541c9dacf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "292c7009-f837-4264-a7dc-214e6d132eb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try :\n",
    "    \n",
    "    news_df = spark.read.parquet(new_data_path)\n",
    "    news_df = news_df.na.drop()  \n",
    "\n",
    "    spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "\n",
    "\n",
    "        # Define the transformation based on the condition\n",
    "    news_df = news_df.withColumn(\n",
    "        \"pub_date\",\n",
    "        when(\n",
    "            col(\"link\").contains(\"https://www.amazon.science\"),\n",
    "            to_timestamp(unix_timestamp(col(\"pub_date\"), \"EEE, dd MMM yyyy HH:mm:ss 'GMT'\").cast(\"timestamp\"))\n",
    "        ).otherwise(\n",
    "            to_timestamp(unix_timestamp(col(\"pub_date\"), \"EEE, dd MMM yyyy HH:mm:ss Z\").cast(\"timestamp\"))\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    content_list = news_df.select(\"content\").rdd.flatMap(lambda x: x).collect()\n",
    "    news_chunks = list(map(process_content, content_list))\n",
    "\n",
    "    structured_news_chunks = create_chunks_list(news_df,news_chunks)\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"chunk_id\", StringType(), True),\n",
    "        StructField(\"guid\", StringType(), True),\n",
    "        StructField(\"pub_date\", DateType(), True),\n",
    "        StructField(\"chunk_text\", StringType(), True),\n",
    "        StructField(\"start_index\", IntegerType(), True),\n",
    "        StructField(\"end_index\", IntegerType(), True),\n",
    "    ])\n",
    "\n",
    "    chunks_df = spark.createDataFrame(structured_news_chunks,schema=schema)\n",
    "except Exception as e:\n",
    "    logging.error(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d90f51b-8d0f-4a88-b31f-d7105387bf28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Specify the file path in your ADLS\n",
    "data_path = \"/mnt/bronze-layer/processed_data/data.parquet\"\n",
    "new_chunks_path = \"/mnt/silver-layer/new_chunks.parquet\"\n",
    "\n",
    "# Check if the file exists\n",
    "try:\n",
    "    files = dbutils.fs.ls(data_path)\n",
    "\n",
    "    #if it passed this step so there is a file \n",
    "\n",
    "    news_df.write.mode(\"append\").parquet(data_path)\n",
    "except Exception as e:\n",
    "    news_df.write.parquet(data_path)\n",
    "\n",
    "# store the chunks\n",
    "chunks_df.write.parquet(new_chunks_path)\n",
    "\n",
    "# remove the new_data file because it have been processed \n",
    "dbutils.fs.rm(new_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bbbc1d75-a474-4174-96b8-91e2ae58f8f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze2silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
